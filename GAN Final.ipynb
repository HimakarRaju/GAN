{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d21e0572-33ab-462b-b23e-10a771c0f5d2",
   "metadata": {},
   "source": [
    "GAN\n",
    "\n",
    "resizing all images\n",
    "CycleGAN typically requires images of a fixed size (e.g., 256x256). Use the following script to resize all images in a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57ed3845-4d0f-4cad-8522-97d148285ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def convert_images(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Convert images in the input directory to RGB and save them as JPEG in the output directory.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                with Image.open(file_path) as img:\n",
    "                    # Convert image to RGB if it's not already in that mode\n",
    "                    if img.mode not in (\"RGB\", \"L\"):\n",
    "                        img = img.convert(\"RGB\")\n",
    "                    \n",
    "                    # Create corresponding output path\n",
    "                    relative_path = os.path.relpath(root, input_dir)\n",
    "                    save_dir = os.path.join(output_dir, relative_path)\n",
    "                    os.makedirs(save_dir, exist_ok=True)\n",
    "                    \n",
    "                    # Save image as JPEG\n",
    "                    save_path = os.path.join(save_dir, f\"{os.path.splitext(file)[0]}.jpg\")\n",
    "                    img.save(save_path, \"JPEG\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "def move_processed_images(output_dir, original_dir):\n",
    "    \"\"\"\n",
    "    Move processed images back to their respective original folders.\n",
    "    \"\"\"\n",
    "    for root, _, files in os.walk(output_dir):\n",
    "        for file in files:\n",
    "            src_path = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(root, output_dir)\n",
    "            dest_dir = os.path.join(original_dir, relative_path)\n",
    "            dest_path = os.path.join(dest_dir, file)\n",
    "            \n",
    "            # Ensure the destination folder exists\n",
    "            os.makedirs(dest_dir, exist_ok=True)\n",
    "            \n",
    "            # Move the file\n",
    "            shutil.move(src_path, dest_path)\n",
    "\n",
    "# Set the paths for your dataset\n",
    "input_directory = \"dataset\"\n",
    "output_directory = \"processed_dataset\"\n",
    "\n",
    "# Step 1: Convert the images\n",
    "convert_images(input_directory, output_directory)\n",
    "\n",
    "# Step 2: Move the processed images back to their respective folders\n",
    "move_processed_images(output_directory, input_directory)\n",
    "\n",
    "# Step 3: Clean up the temporary processed folder\n",
    "shutil.rmtree(output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f9d241a-a45a-45cc-a88f-4b228250ce19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def resize_images(input_dir, output_dir, size=(256, 256)):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for category in os.listdir(input_dir):\n",
    "        category_path = os.path.join(input_dir, category)\n",
    "        output_category_path = os.path.join(output_dir, category)\n",
    "        if not os.path.exists(output_category_path):\n",
    "            os.makedirs(output_category_path)\n",
    "\n",
    "        for filename in os.listdir(category_path):\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "                img_path = os.path.join(category_path, filename)\n",
    "                output_path = os.path.join(output_category_path, filename)\n",
    "                try:\n",
    "                    img = Image.open(img_path)\n",
    "                    img = img.resize(size)\n",
    "                    img.save(output_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "resize_images(\"dataset\", \"processed_dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeea219-c9d0-467e-b955-dff712852b02",
   "metadata": {},
   "source": [
    "DataSet Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ab83314-5007-46c7-9d0c-54d14779ad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        \n",
    "        # Collect all images from subdirectories\n",
    "        for category in os.listdir(root_dir):\n",
    "            category_path = os.path.join(root_dir, category)\n",
    "            if os.path.isdir(category_path):\n",
    "                for img_name in os.listdir(category_path):\n",
    "                    self.image_paths.append(os.path.join(category_path, img_name))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# Transform and DataLoader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize images\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "dataset = WeatherDataset(root_dir=\"dataset\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3ddd04-272c-4994-bbc2-3391c3e5f1b3",
   "metadata": {},
   "source": [
    "Step 3: \n",
    "\n",
    "Define GAN Architecture\n",
    "\n",
    "\n",
    "Basic GAN Components:\n",
    "\n",
    "Generator (G):\n",
    "Takes random noise as input and generates images.\n",
    "\n",
    "Discriminator (D):\n",
    "Differentiates between real and generated images.\n",
    "\n",
    "Generator Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acc8c068-4c5f-4c81-beae-e6cb5f8f579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim=100):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 3 * 256 * 256),  # Output image size\n",
    "            nn.Tanh()  # Normalize output to [-1, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.main(z).view(-1, 3, 256, 256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f511a185-610e-48b0-af5b-9b7244ddb21f",
   "metadata": {},
   "source": [
    "Discriminator Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a7db769-2c3f-446c-a13f-cdf996fa57cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(3 * 256 * 256, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()  # Output probability of real/fake\n",
    "        )\n",
    "    \n",
    "    def forward(self, img):\n",
    "        return self.main(img.view(img.size(0), -1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b015b096-6edb-49e8-be57-6117364e4016",
   "metadata": {},
   "source": [
    "Step 4: Training the GAN\n",
    "Define Loss and Optimizers:\n",
    "\n",
    "Binary Cross-Entropy Loss for Discriminator.\n",
    "Adam optimizer for both Generator and Discriminator.\n",
    "Training Loop:\n",
    "\n",
    "Alternate between training G and D.\n",
    "Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "290f55a1-9fdf-4cb1-bb5d-5d0ab1ddb85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if CUDA is working\n",
    "print(torch.cuda.current_device())  # Should show your GPU ID (if available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc31ab-b7e8-4142-ba38-23acfd8742e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████| 215/215 [22:44<00:00,  6.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] | D Loss: 0.6313520669937134 | G Loss: 0.7607000470161438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████| 215/215 [23:48<00:00,  6.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100] | D Loss: 0.7778439521789551 | G Loss: 0.8395625352859497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████| 215/215 [27:25<00:00,  7.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100] | D Loss: 0.6752406358718872 | G Loss: 1.2444467544555664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▉                     | 9/215 [01:02<26:37,  7.76s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize models\n",
    "G = Generator(noise_dim=100).cuda()\n",
    "D = Discriminator().cuda()\n",
    "\n",
    "# Loss and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Training\n",
    "epochs = 100\n",
    "noise_dim = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for real_imgs in tqdm(dataloader):\n",
    "        real_imgs = real_imgs.cuda()\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        real_labels = torch.ones(real_imgs.size(0), 1).cuda()\n",
    "        fake_labels = torch.zeros(real_imgs.size(0), 1).cuda()\n",
    "        \n",
    "        real_outputs = D(real_imgs)\n",
    "        real_loss = criterion(real_outputs, real_labels)\n",
    "        \n",
    "        noise = torch.randn(real_imgs.size(0), noise_dim).cuda()\n",
    "        fake_imgs = G(noise)\n",
    "        fake_outputs = D(fake_imgs.detach())\n",
    "        fake_loss = criterion(fake_outputs, fake_labels)\n",
    "        \n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        fake_outputs = D(fake_imgs)\n",
    "        g_loss = criterion(fake_outputs, real_labels)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] | D Loss: {d_loss.item()} | G Loss: {g_loss.item()}\")\n",
    "\n",
    "    # Save generated images every few epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            sample_noise = torch.randn(16, noise_dim).cuda()\n",
    "            generated_images = G(sample_noise).cpu()\n",
    "            torchvision.utils.save_image(generated_images, f\"generated_epoch_{epoch+1}.png\", normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1807923-242d-477c-90b8-48faa640725c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
